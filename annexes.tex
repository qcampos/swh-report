\section{Software Heritage Archiver (Version
1)}\label{software-heritage-archiver-version-1}

The Software Heritage (SWH) Archiver is responsible for backing up SWH
objects as to reduce the risk of losing them.

Currently, the archiver only deals with content objects (i.e., those
referenced by the content table in the DB and stored in the SWH object
storage). The database itself is lively replicated by other means.

\subsection{Requirements}\label{requirements}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Master/slave architecture}
\end{itemize}

There is 1 master copy and 1 or more slave copies of each object. A
retention policy specifies the minimum number of copies that are
required to be ``safe''.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Append-only archival}
\end{itemize}

The archiver treats master as read-only storage and slaves as
append-only storages. The archiver never deletes any object. If removals
are needed, in either master or slaves, they will be dealt with by other
means.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Asynchronous archival.}
\end{itemize}

Periodically (e.g., via cron), the archiver runs, produces a list of
objects that need to be copied from master to slaves, and starts copying
them over. Very likely, during any given archival run other new objects
will be added to master; it will be the responsibility of \emph{future}
archiver runs, and not the current one, to copy new objects over.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Integrity at archival time.}
\end{itemize}

Before archiving objects, the archiver performs suitable integrity
checks on them. For instance, content objects are verified to ensure
that they can be decompressed and that their content match their
(checksum-based) unique identifiers. Corrupt objects will not be
archived and suitable errors reporting about the corruption will be
emitted.

Note that archival-time integrity checks are \emph{not meant to replace
periodic integrity checks} on both master and slave copies.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Parallel archival}
\end{itemize}

Once the list of objects to be archived has been identified, it SHOULD
be possible to archive objects in parallel w.r.t. one another.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Persistent archival status}
\end{itemize}

The archiver maintains a mapping between objects and the locations where
they are stored. Locations are the set \{master, slave\_1, \ldots{},
slave\_n\}.

Each pair is also associated to the following information:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{status}: 3-state: \emph{missing} (copy not present at
  destination), \emph{ongoing} (copy to destination ongoing),
  \emph{present} (copy present at destination)
\item
  \textbf{mtime}: timestamp of last status change. This is either the
  destination archival time (when status=present), or the timestamp of
  the last archival request (status=ongoing); the timestamp is undefined
  when status=missing.
\end{itemize}

\subsection{Architecture}\label{architecture}

The archiver is comprised of the following software components:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  archiver director
\item
  archiver worker
\item
  archiver copier
\end{itemize}

\subsubsection{Archiver director}\label{archiver-director}

The archiver director is run periodically, e.g., via cron.

Runtime parameters:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  execution periodicity (external)
\item
  retention policy
\item
  archival max age
\item
  archival batch size
\end{itemize}

At each execution the director:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  for each object: retrieve its archival status
\item
  for each object that is in the master storage but has fewer copies
  than those requested by the retention policy:
\item
  if status=ongoing and mtime is not older than archival max age then
  continue to next object
\item
  check object integrity (e.g., with
  swh.storage.ObjStorage.check(obj\_id))
\item
  mark object as needing archival
\item
  group objects in need of archival in batches of archival batch size
\item
  for each batch:
\item
  set status=ongoing and mtime=now() for each object in the batch
\item
  spawn an archive worker on the whole batch (e.g., submitting the
  relevant celery task)
\end{enumerate}

Note that if an archiver worker task takes a long time (t \textgreater{}
archival max age) to complete, it is possible for another task to be
scheduled on the same batch, or an overlapping one.

\subsubsection{Archiver worker}\label{archiver-worker}

The archiver is executed on demand (e.g., by a celery worker) to archive
a given set of objects.

Runtime parameters:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  objects to archive
\end{itemize}

At each execution a worker:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  create empty map \{ destinations -\textgreater{} objects that need to
  be copied there \}
\item
  for each object to archive:
\item
  retrieve current archive status for all destinations
\item
  update the map noting where the object needs to be copied
\item
  for each destination:
\item
  look up in the map objects that need to be copied there
\item
  copy all objects to destination using the copier
\item
  set status=present and mtime=now() for each copied object
\end{enumerate}

Note that:

\begin{itemize}
\item
  In case multiple jobs where tasked to archive the same of overlapping
  objects, step (2.2) might decide that some/all objects of this batch
  no longer need to be archived to some/all destinations.
\item
  Due to parallelism, it is also possible that the same objects will be
  copied over at the same time by multiple workers.
\end{itemize}

\subsubsection{Archiver copier}\label{archiver-copier}

The copier is run on demand by archiver workers, to transfer file
batches from master to a given destination.

The copier transfers all files together with a single network
connection. The copying process is atomic at the file granularity (i.e.,
individual files might be visible on the destination before \emph{all}
files have been transferred) and ensures that \emph{concurrent transfer
of the same files by multiple copier instances do not result in
corrupted files}. Note that, due to this and the fact that timestamps
are updated by the director, all files copied in the same batch will
have the same mtime even though the actual file creation times on a
given destination might differ.

As a first approximation, the copier can be implemented using rsync, but
a dedicated protocol can be devised later. In the case of rsync, one
should use --files-from to list the file to be copied. Rsync atomically
renames files one-by-one during transfer; so as long as --inplace is
\emph{not} used, concurrent rsync of the same files should not be a
problem.

\subsection{DB structure}\label{db-structure}

Postgres SQL definitions for the archival status:

\begin{verbatim}
CREATE DOMAIN archive_id AS TEXT;

CREATE TABLE archives (
  id   archive_id PRIMARY KEY,
  url  TEXT
);

CREATE TYPE archive_status AS ENUM (
  'missing',
  'ongoing',
  'present'
);

CREATE TABLE content_archive (
  content_id  sha1 REFERENCES content(sha1),
  archive_id  archive_id REFERENCES archives(id),
  status      archive_status,
  mtime       timestamptz,
  PRIMARY KEY (content_id, archive_id)
);
\end{verbatim}

\section{Software Heritage Archiver (Version
2)}\label{software-heritage-archiver-version-2}

The Software Heritage (SWH) Archiver is responsible for backing up SWH
objects as to reduce the risk of losing them.

Currently, the archiver only deals with content objects (i.e., those
referenced by the content table in the DB and stored in the SWH object
storage). The database itself is lively replicated by other means.

\subsection{Requirements}\label{requirements-1}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Peer-to-peer topology}
\end{itemize}

Every storage involved in the archival process can be used as a source
or a destination for the archival, depending on the blobs it contains. A
retention policy specifies the minimum number of copies that are
required to be ``safe''.

Althoug the servers are totally equals the coordination of which content
should be copied and from where to where is centralized.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Append-only archival}
\end{itemize}

The archiver treats involved storages as append-only storages. The
archiver never deletes any object. If removals are needed, they will be
dealt with by other means.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Asynchronous archival.}
\end{itemize}

Periodically (e.g., via cron), the archiver runs, produces a list of
objects that need to have more copies, and starts copying them over. The
decision of which storages are choosen to be sources and destinations
are not up to the storages themselves.

Very likely, during any given archival run, other new objects will be
added to storages; it will be the responsibility of \emph{future}
archiver runs, and not the current one, to copy new objects over if
needed.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Integrity at archival time.}
\end{itemize}

Before archiving objects, the archiver performs suitable integrity
checks on them. For instance, content objects are verified to ensure
that they can be decompressed and that their content match their
(checksum-based) unique identifiers. Corrupt objects will not be
archived and suitable errors reporting about the corruption will be
emitted.

Note that archival-time integrity checks are \emph{not meant to replace
periodic integrity checks}.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Parallel archival}
\end{itemize}

Once the list of objects to be archived has been identified, it SHOULD
be possible to archive objects in parallel w.r.t. one another.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Persistent archival status}
\end{itemize}

The archiver maintains a mapping between objects and the locations where
they are stored. Locations are the set \{master, slave\_1, \ldots{},
slave\_n\}.

Each pair is also associated to the following information:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{status}: 3-state: \emph{missing} (copy not present at
  destination), \emph{ongoing} (copy to destination ongoing),
  \emph{present} (copy present at destination), \emph{corrupted}
  (content detected as corrupted during an archival).
\item
  \textbf{mtime}: timestamp of last status change. This is either the
  destination archival time (when status=present), or the timestamp of
  the last archival request (status=ongoing); the timestamp is undefined
  when status=missing.
\end{itemize}

\subsection{Architecture}\label{architecture-1}

The archiver is comprised of the following software components:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  archiver director
\item
  archiver worker
\item
  archiver copier
\end{itemize}

\subsubsection{Archiver director}\label{archiver-director-1}

The archiver director is run periodically, e.g., via cron.

Runtime parameters:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  execution periodicity (external)
\item
  retention policy
\item
  archival max age
\item
  archival batch size
\end{itemize}

At each execution the director:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  for each object: retrieve its archival status
\item
  for each object that has fewer copies than those requested by the
  retention policy:
\item
  mark object as needing archival
\item
  group objects in need of archival in batches of
  \texttt{archival batch size}
\item
  for each batch:
\item
  spawn an archive worker on the whole batch (e.g., submitting the
  relevant celery task)
\end{enumerate}

\subsubsection{Archiver worker}\label{archiver-worker-1}

The archiver is executed on demand (e.g., by a celery worker) to archive
a given set of objects.

Runtime parameters:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  objects to archive
\item
  archival policies (retention \& archival max age)
\end{itemize}

At each execution a worker:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  for each object in the batch
\item
  check that the object still need to be archived (\#present copies
  \textless{} retention policy)
\item
  if an object has status=ongoing but the elapsed time from task
  submission is less than the \emph{archival max age}, it count as
  present, as we assume that it will be copied in the futur. If the
  delay is elapsed, otherwise, is count as a missing copy.
\item
  for each object to archive:
\item
  retrieve current archive status for all destinations
\item
  create a map noting where the object is present and where it can be
  copied
\item
  Randomly choose couples (source, destination), where destinations are
  all differents, to make enough copies
\item
  for each (content, source, destination):
\item
  Join the contents by key (source, destination) to have a map
  \{(source, destination) -\textgreater{} {[}contents{]}\}
\item
  for each (source, destination) -\textgreater{} contents
\item
  for each content in content, check its integrity on the source storage

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    if the object if corrupted or missing

    \begin{itemize}
    \itemsep1pt\parskip0pt\parsep0pt
    \item
      update its status in the database
    \item
      remove it from the current contents list
    \end{itemize}
  \end{itemize}
\item
  start the copy of the batces by launching for each transfer tuple a
  copier

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    if an error occured on one of the content that should have been
    valid, consider the whole batch as a failure.
  \end{itemize}
\item
  set status=present and mtime=now for each successfully copied object
\end{enumerate}

Note that:

\begin{itemize}
\item
  In case multiple jobs where tasked to archive the same of overlapping
  objects, step (1) might decide that some/all objects of this batch no
  longer need to be archived.
\item
  Due to parallelism, it is possible that the same objects will be
  copied over at the same time by multiple workers. Also, the same
  object could end having more copies than the minimal number required.
\end{itemize}

\subsubsection{Archiver copier}\label{archiver-copier-1}

The copier is run on demand by archiver workers, to transfer file
batches from a given source to a given destination.

The copier transfers files one by one. The copying process is atomic at
the file granularity (i.e., individual files might be visible on the
destination before \emph{all} files have been transferred) and ensures
that \emph{concurrent transfer of the same files by multiple copier
instances do not result in corrupted files}. Note that, due to this and
the fact that timestamps are updated by the worker, all files copied in
the same batch will have the same mtime even though the actual file
creation times on a given destination might differ.

The copier is implemented using the ObjStorage API for the sources and
destinations.

\subsection{DB structure}\label{db-structure-1}

Postgres SQL definitions for the archival status:

\begin{verbatim}
-- Those names are sample of archives server names
CREATE TYPE archive_id AS ENUM (
  'uffizi',
  'banco'
);

CREATE TABLE archive (
  id   archive_id PRIMARY KEY,
  url  TEXT
);

CREATE TYPE archive_status AS ENUM (
  'missing',
  'ongoing',
  'present',
  'corrupted'
);

CREATE TABLE content_archive (
  content_id sha1 unique,
  copies jsonb
);
\end{verbatim}

Where the content\_archive.copies field is of type jsonb and contains
datas about the storages that contains (or not) the content represented
by the sha1

\begin{verbatim}
{
    "$schema": "http://json-schema.org/schema#",
    "title": "Copies data",
    "description": "Data about the presence of a content into the storages",
    "type": "object",
    "Patternproperties": {
        "^[a-zA-Z1-9]+$": {
            "description": "archival status for the server named by key",
            "type": "object",
            "properties": {
                "status": {
                    "description": "Archival status on this copy",
                    "type": "string",
                    "enum": ["missing", "ongoing", "present", "corrupted"]
                },
                "mtime": {
                    "description": "Last time of the status update",
                    "type": "float"
                }
            }
        }
    },
    "additionalProperties": false
}
\end{verbatim}

\section{Software Heritage Vault
\label{annexe-vault}}\label{software-heritage-vault}

Software source code \textbf{objects}---e.g., individual source code
files, tarballs, commits, tagged releases, etc.---are stored in the
Software Heritage (SWH) Archive in fully deduplicated form. That allows
direct access to individual artifacts but require some preparation,
usually in the form of collecting and assembling multiple artifacts in a
single \textbf{bundle}, when fast access to a set of related artifacts
(e.g., the snapshot of a VCS repository, the archive corresponding to a
Git commit, or a specific software release as a zip archive) is
required.

The \textbf{Software Heritage Vault} is a cache of pre-built source code
bundles which are assembled opportunistically retrieving objects from
the Software Heritage Archive, can be accessed efficiently, and might be
garbage collected after a long period of non-use.

\subsection{Requirements}\label{requirements-2}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Shared cache}
\end{itemize}

The vault is a cache shared among the various origins that the SWH
archive tracks. If the same bundle, originally coming from different
origins, is requested, a single entry for it in the cache shall exist.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{Efficient retrieval}
\end{itemize}

Where supported by the desired access protocol (e.g., HTTP) it should be
possible for the vault to serve bundles efficiently (e.g., as static
files served via HTTP, possibly further proxied/cached at that level).
In particular, this rules out building bundles on the fly from the
archive DB.

\subsection{API}\label{api}

All URLs below are meant to be mounted at API root, which is currently
at \url{https://archive.softwareheritage.org/api/1/}. Unless otherwise
stated, all API endpoints respond on HTTP GET method.

\subsection{Object identification}\label{object-identification}

The vault stores bundles corresponding to different kinds of objects.
The following object kinds are supported:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  directories
\item
  revisions
\item
  repository snapshots
\end{itemize}

The URL fragment \texttt{:objectkind/:objectid} is used throughout the
vault API to fully identify vault objects. The syntax and meaning of
:objectid for the different object kinds is detailed below.

\subsubsection{Directories}\label{directories}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  object kind: directory
\item
  URL fragment: directory/:sha1git
\end{itemize}

where :sha1git is the directory ID in the SWH data model.

\subsubsection{Revisions}\label{revisions}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  object kind: revision
\item
  URL fragment: revision/:sha1git
\end{itemize}

where :sha1git is the revision ID in the SWH data model.

\subsubsection{Repository snapshots}\label{repository-snapshots}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  object kind: snapshot
\item
  URL fragment: snapshot/:sha1git
\end{itemize}

where :sha1git is the snapshot ID in the SWH data model. (\textbf{TODO}
repository snapshots don't exist yet as first-class citizens in the SWH
data model; see References below.)

\subsection{Cooking}\label{cooking}

Bundles in the vault might be ready for retrieval or not. When they are
not, they will need to be \textbf{cooked} before they can be retrieved.
A cooked bundle will remain around until it expires; at that point it
will need to be cooked again before it can be retrieved. Cooking is
idempotent, and a no-op in between a previous cooking operation and
expiration.

To cook a bundle:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  POST /vault/:objectkind/:objectid
\end{itemize}

Request body: \textbf{TODO} something here in a JSON payload that would
allow notifying the user when the bundle is ready.

Response: 201 Created

\subsection{Retrieval}\label{retrieval}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  GET /vault/:objectkind
\end{itemize}

(paginated) list of all bundles of a given kind available in the vault;
see Pagination. Note that, due to cache expiration, objects might
disappear between listing and subsequent actions on them.

Examples:

\begin{itemize}
\item
  GET /vault/directory
\item
  GET /vault/revision
\item
  GET /vault/:objectkind/:objectid
\end{itemize}

Retrieve a specific bundle from the vault.

Response:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  200 OK: bundle available; response body is the bundle
\item
  404 Not Found: missing bundle; client should request its preparation
  (see Cooking)
\end{itemize}

\subsection{References}\label{references}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \href{https://wiki.softwareheritage.org/index.php?title=User:StefanoZacchiroli/Repository_snapshot_objects}{Repository
  snapshot objects}
\item
  Amazon Web Services,
  \href{http://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-api.html}{API
  Reference for Amazon Glacier}; specifically
  \href{http://docs.aws.amazon.com/amazonglacier/latest/dev/job-operations.html}{Job
  Operations}
\end{itemize}

\subsection{TODO}\label{todo}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \textbf{TODO} pagination using HATEOAS
\item
  \textbf{TODO} authorization: the cooking API should be somehow
  controlled to avoid obvious abuses (e.g., let's cache everything)
\item
  \textbf{TODO} finalize repository snapshot proposal
\end{itemize}

\section{Schéma relationnel}\label{annexe-db}

\begin{figure}
\includegraphics[angle=90, scale=3]{swh-data-model.pdf}
\caption{Schéma relationnel de la base de donnée Software Heritage}
\end{figure}
